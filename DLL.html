import math

class SimpleLogReg:
    def __init__(self, lr=0.1, epochs=200):
        self.lr = lr
        self.epochs = epochs

    def sigmoid(self, z):
        if z > 700:
            return 1.0
        if z < -700:
            return 0.0
        return 1 / (1 + math.exp(-z))

    def fit(self, X, y):
        m = len(X)
        n = len(X[0])
        self.w = [0]*n
        self.b = 0

        for epoch in range(self.epochs):
            y_pred = []
            for xi in X:
                z = self.b + sum(self.w[j]*xi[j] for j in range(n))
                y_pred.append(self.sigmoid(z))

            
            # COST COMPUTATION (moved inside here!)
            
            eps = 1e-12
            cost = 0
            for i in range(m):
                h = max(eps, min(1 - eps, y_pred[i]))
                cost += -(y[i] * math.log(h) + (1 - y[i]) * math.log(1 - h))
            cost /= m

            # EPOCH PRINTING
            if epoch % 50 == 0:
                print("Epoch:", epoch, "Cost:", cost)

            # gradients
            dw = [0]*n
            db = 0
            for i in range(m):
                error = y_pred[i] - y[i]
                db += error
                for j in range(n):
                    dw[j] += error * X[i][j]

            # average
            db /= m
            dw = [d/m for d in dw]

            # update
            self.b -= self.lr * db
            for j in range(n):
                self.w[j] -= self.lr * dw[j]

    def predict(self, X, threshold=0.5):
        preds = []
        for xi in X:
            z = self.b + sum(self.w[j]*xi[j] for j in range(len(self.w)))
            prob = self.sigmoid(z)
            preds.append(1 if prob >= threshold else 0)
        return preds


# TESTING
X = [[1],[1.5],[2],[2.5],[4.5],[5],[5.5],[6]]
y = [0,0,0,0,1,1,1,1]

model = SimpleLogReg(lr=0.1, epochs=5000)
model.fit(X, y)

print("Weights:", model.w)
print("Bias:", model.b)
print("Pred:", model.predict([[0.5],[3],[3.5],[7]]))




























import math

class CoreLogisticRegression:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        """Initializes the model's hyperparameters."""
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.theta_0 = 0.0
        self.theta_weights = []
        self.cost_history = []

    def _sigmoid(self, z):
        """Computes the sigmoid function with numerical stability."""
        if z > 700:
            return 1.0
        elif z < -700:
            return 0.0
        else:
            return 1.0 / (1.0 + math.exp(-z))

    def _compute_z(self, x_sample):
        """Computes the linear combination of inputs and weights."""
        z = self.theta_0
        for i in range(len(self.theta_weights)):
            z += self.theta_weights[i] * x_sample[i]
        return z

    def _predict_probability(self, x_sample):
        """Makes a probability prediction for a single sample."""
        z = self._compute_z(x_sample)
        return self._sigmoid(z)

    def _compute_cost(self, y_true, y_pred_probs):
        """Computes the binary cross-entropy (log loss)."""
        m = len(y_true)
        if m == 0:
            return 0.0

        total_cost, epsilon = 0.0, 1e-9
        for i in range(m):
            # Clipping to avoid log(0)
            h = max(epsilon, min(1.0 - epsilon, y_pred_probs[i]))
            cost_sample = -y_true[i] * math.log(h) - (1 - y_true[i]) * math.log(1 - h)
            total_cost += cost_sample

        return total_cost / m

    def _compute_gradients(self, X_data, y_true, y_pred_probs):
        """Computes the gradients of the cost function."""
        m = len(y_true)
        n_features = len(self.theta_weights)

        grad_theta_0 = 0.0
        grad_theta_weights = [0.0] * n_features

        for i in range(m):
            error = y_pred_probs[i] - y_true[i]
            grad_theta_0 += error
            for j in range(n_features):
                grad_theta_weights[j] += error * X_data[i][j]

        grad_theta_0 /= m
        for j in range(n_features):
            grad_theta_weights[j] /= m

        return grad_theta_0, grad_theta_weights

    def fit(self, X_data, y_data, verbose=True):
        """Trains the model using batch gradient descent."""
        n_features = len(X_data[0])

        self.theta_0 = 0.0
        self.theta_weights = [0.0] * n_features
        self.cost_history = []

        for i in range(self.n_iterations):
            y_pred_probs = [self._predict_probability(x) for x in X_data]
            cost = self._compute_cost(y_data, y_pred_probs)
            self.cost_history.append(cost)

            grad_theta_0, grad_theta_weights = self._compute_gradients(
                X_data, y_data, y_pred_probs
            )

            self.theta_0 -= self.learning_rate * grad_theta_0

            for j in range(n_features):
                self.theta_weights[j] -= self.learning_rate * grad_theta_weights[j]

            if verbose and i % (self.n_iterations // 1000) == 0:
                print(f"Iteration {i}: Cost = {cost:.4f}")

    def predict_proba(self, X_data):
        """Predicts probabilities for new data."""
        return [self._predict_probability(x) for x in X_data]

    def predict(self, X_data, threshold=0.5):
        """Predicts class labels (0 or 1) based on a threshold."""
        probabilities = self.predict_proba(X_data)
        return [1 if prob >= threshold else 0 for prob in probabilities]



# TESTING (YOUR REQUESTED INPUT ADDED)


X = [[1], [1.5], [2], [2.5], [4.5], [5], [5.5], [6]]
y = [0, 0, 0, 0, 1, 1, 1, 1]

model = CoreLogisticRegression(learning_rate=0.1, n_iterations=3000)
model.fit(X, y)

print("Weights:", model.theta_weights)
print("Bias:", model.theta_0)

print("Predictions:", model.predict([[0.5], [3], [3.5], [7]]))
